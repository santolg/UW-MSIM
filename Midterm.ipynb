{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 1\n",
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#load data\n",
    "derm_df = pd.read_csv('dermatology.csv', delimiter = '\\t')\n",
    "\n",
    "#correct column names\n",
    "derm_df = derm_df.rename(columns = {\"Family History\":\"Family Hostory\"})\n",
    "derm_df = derm_df.rename(columns = {\"Disappearance\":\"Disapperance\"});\n",
    "\n",
    "#remove rows with '?'/missing value\n",
    "derm_df.drop(derm_df.index[derm_df['Age'] == '?'], inplace=True)\n",
    "\n",
    "#convert columns to numeric\n",
    "derm_df = derm_df.apply(pd.to_numeric)\n",
    "\n",
    "#preview data\n",
    "derm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1048a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f06169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolating independent feature of Age\n",
    "x = derm_df['Age']\n",
    "\n",
    "#isolating class label of Disease\n",
    "y = derm_df['Disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = sm.OLS(y,x).fit()\n",
    "print(lr_model.summary())\n",
    "\n",
    "#MODEL 1: GRADIENT DESCENT\n",
    "def gradient_descent(x, y):\n",
    "    m1 = 0\n",
    "    c1 = 0\n",
    "    epochs = 5000 #define number of iterations\n",
    "    n = len(x)\n",
    "    L = 0.000643 #learning rate\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_Pred = m1 * x + c1\n",
    "        cost = (1/n) * sum([val**2 for val in (y-y_Pred)])\n",
    "        md = -(2/n)*sum(x*(y-y_Pred)) #m derivative\n",
    "        cd = -(2/n)*sum(y-y_Pred) #c derivative\n",
    "        m1 = m1 - L * md\n",
    "        c1 = c1 - L * cd\n",
    "        print (\"m {}, c {}, cost {} iteration {}\".format(m1, c1, cost, i))\n",
    "\n",
    "gradient_descent(x, y)\n",
    "\n",
    "#To minimize the cost function and find the function minimum, I used 5000 iterations with a learning rate of 0.000643.\n",
    "\n",
    "#For derm_df, the optimal intercept is 2.3224138138359405 and the optimal gradient is 0.006776863593425256\n",
    "#this yields the lowest cost function at 2.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2: RANDOM FOREST\n",
    "\n",
    "#isolating independent features(clinical and histopathological attributes)\n",
    "X = derm_df.iloc[0:, 0:33]\n",
    "\n",
    "#isolating class label\n",
    "y = derm_df.iloc[0:, 34]\n",
    "\n",
    "# normalize feature variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_features = X\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "rfc.fit(X, y.values.ravel())\n",
    "\n",
    "predictions = rfc.predict(X)\n",
    "print(\"RF ACCURACY:\", accuracy_score(y, predictions))\n",
    "print(confusion_matrix(y, predictions))\n",
    "\n",
    "#specified number of trees (n_estimators) as 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ccf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 3: KNN\n",
    "\n",
    "#isolating independent features(clinical and histopathological attributes)\n",
    "X = derm_df.iloc[0:, 0:33]\n",
    "#isolating class label\n",
    "y = derm_df.iloc[0:, 34]\n",
    "\n",
    "# normalize feature variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_features = X\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#create KNN Classifier model\n",
    "knn = KNeighborsClassifier(n_neighbors = 9)\n",
    "\n",
    "#fit  data\n",
    "knn.fit(X, np.ravel(y))\n",
    "\n",
    "#predicting on data\n",
    "predictions = knn.predict(X)\n",
    "\n",
    "#printing Confusion matrix and accuracy scores\n",
    "print('KNN confusion matrix on test data')\n",
    "print(confusion_matrix(y, predictions))\n",
    "\n",
    "print('Accuracy on test data')\n",
    "print(accuracy_score(y, predictions))\n",
    "\n",
    "#To decide k value, I used general rule of thumb of k = sqrt(N)/2. This data set is a bit larger than what we had\n",
    "#used in previous assignments (N=358). Based off this, I selected 9 as the k value. \n",
    "\n",
    "#Accuracy score is 96.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "#MODEL 4: Divisive clustering, K-Means\n",
    "\n",
    "#isolating independent features\n",
    "X = derm_df.iloc[0:, 0:33]\n",
    "\n",
    "# normalize feature variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_features = X\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#will use 6 clusters since there are 6 disease types\n",
    "\n",
    "#K-Means\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 6)\n",
    "y_means = kmeans.fit_predict(X)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "#print values for each of the 6 centroids\n",
    "print(centroids)\n",
    "print('KMeans', y_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34948f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 5: Agglomerative clustering\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.title('Agglomerative clustering')\n",
    "#Ward\n",
    "Dendrogram = sch.dendrogram((sch.linkage(X, method = 'ward')))\n",
    "\n",
    "#will use 6 classes since there are 6 disease types\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters = 6)\n",
    "y_ac = ac.fit_predict(X)\n",
    "\n",
    "print('Agglomerative clustering', y_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 2\n",
    "#load data\n",
    "hc_df = pd.read_csv('hatecrime.csv')\n",
    "\n",
    "#preview data\n",
    "hc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878630a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "print(hc_df.isna().sum())\n",
    "#share_non_citizen(3), hate_crimes_per_100k_fbi(5), avg_hate_crimes_per_100k_fbi(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37154e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with missing values\n",
    "hc_df = hc_df.dropna()\n",
    "\n",
    "# normalize feature variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_features = X\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c236255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. How does income inequality relate to the number of hate crimes and hate incidents? [5 points]\n",
    "\n",
    "#using linear regression because hate crime label is continuous value\n",
    "#isolating class label\n",
    "y = hc_df['hate_crimes_per_100k_splc']\n",
    "#isolating independent feature of Gini index score\n",
    "x = hc_df[['gini_index']]\n",
    "x = sm.add_constant(x)\n",
    "hc_model = sm.OLS(y, x).fit()\n",
    "print(hc_model.summary())\n",
    "\n",
    "#repeating but with avg_hatecrimes_per_100k_fbi label\n",
    "y = hc_df['avg_hatecrimes_per_100k_fbi']\n",
    "#isolating independent feature of Gini index score\n",
    "x = hc_df[['gini_index']]\n",
    "x = sm.add_constant(x)\n",
    "hc_model = sm.OLS(y, x).fit()\n",
    "print(hc_model.summary())\n",
    "\n",
    "#when comparing gini_index to hate_crimes_per_100k_splc and avg_hatecrimes_per_100k_fbi labels, p-value is > 0.05\n",
    "#there is a statistically significant relationship between Gini index score and crime\n",
    "#based off coefficient values, Gini index score and crime rate have a positive relationship\n",
    "#higher income inequality leatds to higher number of hate crimes and incidents\n",
    "\n",
    "#linear equation using gini_index to predict hate_crimes_per_100k_splc\n",
    "#hate_crimes_per_100k_splc = -1.2754  + 3.1512*x\n",
    "\n",
    "#linear equation using gini_index to predict avg_hatecrimes_per_100k_fbi\n",
    "#avg_hatecrimes_per_100k_fbi = -1.5307  + 3.7675*x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. How can we predict the number of hate crimes and hate incidents from race/nature of the population? \n",
    "#scale labels\n",
    "\n",
    "#using linear regression because hate crime label is continuous value\n",
    "#isolating class label\n",
    "y = hc_df['hate_crimes_per_100k_splc']\n",
    "#omitting Gini index score\n",
    "X = hc_df[['median_household_income', 'share_unemployed_seasonal', 'share_population_in_metro_areas', 'share_population_with_high_school_degree', 'share_non_citizen', 'share_white_poverty', 'share_non_white', 'share_voters_voted_trump']]\n",
    "X = sm.add_constant(X)\n",
    "hc_model = sm.OLS(y, X).fit()\n",
    "print(hc_model.summary())\n",
    "#top 3 lowest p-values: share_voters_voted_trump, share_unemployed_seasonal, median_household_income\n",
    "\n",
    "#see how this compares to avg_hatecrimes_per_100k_fbi\n",
    "y = hc_df['avg_hatecrimes_per_100k_fbi']\n",
    "#omitting Gini index score\n",
    "X = hc_df[['median_household_income', 'share_unemployed_seasonal', 'share_population_in_metro_areas', 'share_population_with_high_school_degree', 'share_non_citizen', 'share_white_poverty', 'share_non_white', 'share_voters_voted_trump']]\n",
    "X = sm.add_constant(X)\n",
    "hc_model = sm.OLS(y, X).fit()\n",
    "print(hc_model.summary())\n",
    "#top 3 lowest p-values: share_voters_voted_trump, share_unemployed_seasonal, share_non_citizen\n",
    "\n",
    "\n",
    "#linear equation using top 3 variables stated above (in listed order) to predict hate_crimes_per_100k_splc\n",
    "#hate_crimes_per_100k_splc = -0.0038 - (1.3202*x1) + (3.5613*x2) - (4.89e-06*x3)\n",
    "\n",
    "#linear equation using top 3 variables stated above to predict avg_hatecrimes_per_100k_fbi\n",
    "#avg_hatecrimes_per_100k_fbi = 0.4265 - (-1.3202*x1) + (3.5613*x2) + (0.3391*x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2abd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. How does the number of hate crimes vary across states? Is there any similarity in number of hate incidents (per 100,000 people) between some states than in others — both according to the SPLC after the election and the FBI before it?\n",
    "#isolating class label\n",
    "\n",
    "#add number column to refer to states\n",
    "hc_df['State_Num'] = np.arange(len(hc_df))\n",
    "\n",
    "#Clustering using K Means\n",
    "\n",
    "#isolate and scale attributes\n",
    "X = hc_df[['hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi']]\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X)\n",
    "\n",
    "#K-Means\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "y_means = kmeans.fit_predict(X)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "#print values for each of the 3 centroids\n",
    "print(centroids)\n",
    "print('KMeans', y_means)\n",
    "\n",
    "#visualization\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.title('Divisive Clustering Using K-Means')\n",
    "#visualizing \n",
    "plt.scatter(X['hate_crimes_per_100k_splc'], X['avg_hatecrimes_per_100k_fbi'], c = y_means, cmap = 'rainbow')\n",
    "plt.scatter(centroids[:,0], centroids[:,1], c = 'black', s = 100)\n",
    "plt.show()\n",
    "\n",
    "#Washington DC is shown to have the highest crime rate and incident (cluster 2). \n",
    "\n",
    "#Colorado, Conneticut, Kansas, Maine, Maryland, Michigan, Nevada, West Virginia follow after (cluster 2)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
